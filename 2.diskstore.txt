2. Размерность хранения данных 

Проблема эффективного хранения и доступа к данным
часто несправедливо остаётся за бортом рассмотрения NoSQL движения.

Одним из факторов сдавания позиций реляционных систем является
морально устаревшая алгоритмическая база для хранения данных.

Большинство систем используют B-trees, для упорядоченного хранения
данных, плюс один из вариантов ARIES семейства алгоритмов для
управления версиями.

Б-деревья как структура данных балансируют производительность
вставок и производительность поиска, и при этом "приносят в жертву"
производительность удаления. Этот баланс также неверен
в типичном Web приложении, с динамично устаревающими данными.

В последнее время широкое распостранение получили cache-oblivious
algorithms, которые часто существенно лучше распараллеливаются
в сравнении с традиционными B-trees, что также немаловажно.

Давайте для примера рассмотрим cache-oblivious b-trees.

Также, в области проблематики хранения находится организация
обработки транзакций на одном узле. В зависимости от моделей данных
различия в алгоритмы блокировок при обработке транзакций становятся
немаловажными.

Также давайте рассмотрим такую простую вещь как префиксная компрессия
в б-дереве.

Наксоклько она актуально для json /xml данных? Очень актуально.
Насколько это актуально для нормализованных данных? ну разве что если тип
данных - строка, причём немаленькая (json/xml :)
Стоит ли заморачиваться с префиксной компрессией и MVCC? Алгоритм 
становится непомерно сложным.

Другим примером является блокировки при параллельном доступе к данным.
Модель данных часто диктует реализацию а реализация, в свою очередь, 
ограничиват пространство манёвра в области модели.

Наиболее типичным объектом в документно-ориентированной модели
является документ. Пусть средний размер документа 1 кб. Давайте попробуем
конкрутентно и транзакционно обновить из 1000 потоков *разные*
узлы по разным json paths. Мы получим стагнацию т.к. все 
конкурентные обновления упрутся в блокировку на документе, т.к.
документно-ориентированная база данных часто не расчитана на конкурентные
обновления конкретных полей, а на перезапись всего документа. 


Не стоит забывать также о такой базовой характеристике СУБД 
как latency. С одной стороны, latency - физическая характеристика, 
т.е. лежит на стороне реализации, но с другой стороны, каким образом
обеспечить низкую latency в map/reduce запросе?

Ну а в графовых СУБД без особенностей реализации и модель становется
ненужной и излишней - все графовые операции можно выразить на языке SQL,
то есть в графовых СУБД *реализация* является исходной посылкой
для создания специфичной модели данных.

Все эти примеры демонстрируют что с одной стороны, варианты
реализации образует дополнительную размерность в области СУБД 
решений, а с другой стороны показывает что пространство решений
не может быть непрерывно, и имеет "точки максимума" вокруг потребностей
рынка и возможностей реализации.



Slide 1.
-------

There is a new embedded library coming out every week.
A web engineer is tired of brands. Which one is faster?

tokyoCabinet    tokytyrant      bitcask     leveldb 
            sophia      tokudb          innodb      pbxt
postgresql      HamsterDB       LightningDB   BerkeleyDB 
                        BitCask

Even though the source code is available, sheer
number of engines makes the choice of an engine very
difficult.  The only solution available is to try one out.
Indeed, ther was a mamba.ru talk at nastachku.ru 2013 called

> Практические вопросы использования в нагруженном проекте.

What this talk basically did was to present a number of
open source libraries in the light of a single practical task.
It turned out that LevelDB was the best for this particular
task. 
Does it mean it's going to be best for your tasks? No.
Of course the strategy of trying things out is a "safe" 
one, since the benchmark you construct yourself is likely going
to better reflect your production load than any benchmark out
there you can look at.

But your production load may change - and in fact very likely will
if your project is successful. More data, or different data access
patterns, etc.
    
For the sake of your safe choice, and also for the sake of knowing
the large world around us I've come up with this talk. 

It's a pity even though in most cases you *can* look at what's
inside you can't *understand* what you see. And this is not
because you don't know the programming language the code is
written in.


Slide 2
-------

Let's begin with a simple B-tree.

Why B-tree is used at all?
M-B-N - our input data. M is amount of RAM you have B is block size
on disk, N is the number of elemnts you want to store. 

This is why B tree is called B tree, Because B - is block size,
each leaf in a B tree has size of a Block.

Insert = log B (N), deletion = log B (N), lookup = log B (N)

The good thing about B-tree is that it gives an algorithmical
bound on the cost of each operation. But this bound
is calculated under a few assumptions, which were true 
in 60s, but no longer true:

- size of row is less than block size
- amount of memory is way smaller than the N 

- most importantly, that memory is flat and disk is flat
- cost of access to any piece in memory is equal, and cost
of access of any block on disk is equal 

So the entire cost model in which B-tree shines does
not reflect the modern hardware.
----------------------------------------------------------
The other issue with B-trees which was not relevant before
is variety in access. Any data structure must balance
between:
- data ingestion speed (took 20 minutes to insert data
but 10 days to build indexes)
- data access speed (table w/o indexes -> slow select speed).
- freshness of data (you can build indexes after you insert
data, or maintain them on the fly)

Looks like B-trees are pretty balanced? The cost
of insert is O(log B(N)), select and delete are the same.

But what about: 
- insert vs. select rate - 80/20 - can we find a better
data structure for that? 
- bulk operations - range scans, data loads, deletes,
- data locality: most recent data is accessed first
---------------------------------------------------------
Slide 2: The idea of cache-oblivious algorithms

Mentioned in the first slide, modern hardware has
layers of memory: on-chip L1/L2 caches, NUMA is
non-*uniform* memory architecture, disk controller cache
is built around caching a few *tracks*, whatever they 
happen to contain.

The block transfer model has never been an ideal due
to internal fragmentation, but at least it reflected
reality of rotating disks, whereas there is no such
thing as BLOCK in SSD storage - access cost is flat.

So a new cost model for algorithms is necessary.

Since there are so many layers of caches, it's
necessary to find a model that does not depend
on any one cache size in particular, but
efficiently utilizes all caches. Sometimes
it is achieved by making a single piece of 
data the algorithm operates with << smaller than
the cache size, sometimes, on the opposite, >> than
the cache size.

Then the complexity of the algorithm (cost model) can be expressed
simply as a function of data set size N and cache miss count
proportional to N it incurs (this is in turn a function of cache
size, but algorithm cost estimate formula doesn't change with
change of size of the cache).
It's hard and not always possible. But
this is what cache-oblivious algorithms are about.

oblivious - lacking awareness of.

Let's consider a most basic example of cache-oblivious
algorithm using a cache-oblivious matrix  transposition.

A more interesting example - matrix multiplication:

It is based on block-multiplication formula for matrices A and B,
resulting matrix C, the matrices are all square of size n:


BLOCK - MULT ( A, B, C, n)
1 for i = 1 to n/s
2   do for j = 1 to n/s
3     do for k = 1 to n/s
4       do ORDINRARY- MULT (Aik , Bkj , Cij , s)

s - denotes the size of the block 

As can be seen from these two examples, most cache-oblivious
algorithms are built on divide-and-conquer principle, and 
thus can be parallelized well (compare with insertion-sort,
for example, good luck making it parallel).

Pictures: matrix trasnposition and multiplication
------------------------------------------------

Slide 3
-------

The iog structured merge trees.
------------------------------------

Log structures merge trees were described
in 1996 paper, (O'Neil & others), before the
concept of cache-oblivious data structures was introduced by
Prokop in 1999, but to a large extent repeat
ideas of Prokop.

The basic idea is that instead of a single
tree, you get a set of cascading ordered data structures, 
with "merge trains" running between them: whenever
a level 0 structure becomes full, it's emptied to
level 1, and whenever level 1 becomes full, it is emptied
into level 2.

Since each level is never modified, only inserted
into or merged with the previous level, it's not
possible to physically delete data from an LSM 
tree - so deletes instead are represented by
tombstone records, take space, and deleted records
simply merge down the levels until they reach
the last level, in which they are finally pruned
away.

Note, LSM tree is not yet cache-oblivius: you do get
to set the sizes of levels, according to your hardware and
load, but it already is block-less.

What are the advantages of the LSM tree?
----------------------------------------

- it favours accesses to the last inserted data
- it doesn't have internal fragmentation, since
it doesn't use small (4k-8k) blocks of B-trees
- it is often made append-only, which is a nice
property if you also need MVCC
- perhaps part of general write-over-read
friendliness, but worth noting separately 
that it is batch deletes/inserts/etc friendly

What are the disadvantages: 
---------------------------
- basically, insert cost is traded for select cost.
The select has to look up in a series of cascading
trees instead of just one trees. Modern LSM 
tree implementations use Bloom-filters and other
techniques to mitigate that. It is thus not
very friendly for update-like workloads.

- although they remove internal fragmentation of Block-
oriented storage, the structure introduce write amplification. The
size of your on-disk tree is up to 2x of your data set size, plus,
on average, each record is written to every level 
This is often mitigated by compression in advanced 
storages.

- range scans are also harder, since
 you got to look through many trees instead of one.

- there is an "overflow cascade" problem,
which, if not addressed, can make a single
insert cost very high. If not addressed, this
may render this algorithm unusable (as we will
see later, it is a problem of all cascading data structures,
cache-oblivious a well).

Slide 4
-------

LevelDB as one example of a cascading tree
implementation.

4M block size. Each block (called memtable
in memory and sorted string table on disk)
contains changes for a range of keys.
The memtable is sorted, but is represented
by a log file on disk. 

Level 0 is called "young" files. there
can be up to 4 young files on the level.
Young level files naturally have
overlapping ranges. lover levels dont'
have overlapping ranges.

Each level therefore is a collection of 4M
blocks. Or, alternatively, block size
can be set to grow with level. Each level
is ~10x larger than the prvious one.
So LevelDB "amortizes" merges between
layers since it only merges the modified
blocks from the previous level.

The merge for level-1+ only takes
one file from the previous level and
pushes it to the next level, merging
with (possible multiple) files on the next level.

Compactions for a particular level rotate through the key space.
In more detail, for each level L, we remember the ending key of
the last compaction at level L. The next compaction for level L
will pick the first file that starts after this key (wrapping
around to the beginning of the key space if there is no such
file). 

To skip level look ups it uses a bloom filter
associated with each file.

LevelDB is doing almost nothing to amortize cascading merges
(well, except that 2M range segregation that is built-into the
frame of the algorithm), so it's performance is unstable.

Picture: leveldb  insert latency
Picture: leveldb illustration

Check out HyperLevelDB, a patched leveldb
used in hypertable and a few other projects.

Facebook is doing leveldb++ - ask Mark Callaghan
about it, he's around here.

Slide 5
-------

Cache oblivious lookahead arrays
--------------------------------

Cache oblivious lookahead arrays were developed
in MIT in 2000x and were first implemented
in the closed-source, patented storate engine
for MySQL, TokuDB. Right now this storage engine
is also a possible MongoDB backend, and the
technology became open source in 2013.

The algorithms were jointly created by Kuzmaul, Colton & et al.

The basic idea is that instead of a cascading
set of B-trees, you get a cascade of sorted 
arrays.

Show pictures of how it is done.

The insert cost into this data structure
is O (lg(N)/B) - which is way less than
O(logB(N)).

To best understand cascading of these data
structures imagine a binary representation
of an integer, where each bit represents
a level in a cascade:

0
1
01
11
001
101
011
111
0001

 - it can be shown that each addition
flips on average no more than 1 + o(bit count)
bits - this is what makes these trees so efficient.

But this is on average, in practice there is
the same cascading merge issue as in lsm 
trees, so amortized algorithms must be used 
to reduce spikes in latency. 

Picture - insert into COLA

Part 2
------

Simple popular SELECT-friendy storage algorithms
------------------------------------------------

We have discussed cache oblivious algorithms and
LSM trees, and discovered that they have two major
problems, which make them hard to implement (and
hence no good open source immplementations are available):

- insert latency can vary drastically unless
inserts are amortized - not good for web app

- select speed is preferred for insert speed
 - also not good for a very large number of web apps.

Important to distinguish simple key-value stores
and relational database with secondary keys.

With relational databases with secondary keys
insert cost can be prohibitive if there are 
a lot of B-tree or other type secondary keys, 
thus an index is not created, unless LSM
or COLA can be used for it. But with key-value
you only have a primary key, so the argument
of being able to maintain more indexes is not relevant.

Strangely enough, most key-value stores (cassandra,
hadoop, BigTable) use LSM and actually struggle with it.

Of course, the apps with 80/20 insert/select ratio
these structures are by far unbeatable (and in fact
are assymptotically optimal, alghough seriously
this is academic bullshit since assymptotically 
unoptimal structures can be made to work faster
in practice than assymptotically optimal ones).

Now let's take a look at exactly such structures.

Slide 6
-------

Bitcask

Bitcask simply has a full key cache and 
in each slot in the cache stores an offset to a
value on disk. The cache is represented as a hash
table, so no range scans. The hash
indexes thus an append-only file.
The append-only file is compacted over time
by a separate process.

This assumes that your RAM is infinite -
which is in practice not true, but !

In many cases RAM being 1/10 or 1/5
of the data set is very real and very practical,
thus making the cost estiamations irrelevant.

It is an example, showing  that it's important to
always keep in mind when comparing various data
structures - what are the values of *your* M, N and B?

Slide 7
-------

lmdb

I have no fucking clue what this is but
it is known because it'a part of openLdap.

Make a reverence to it to provide a good
intro to Sophia.

Slide 8
-------

Sophia

DESIGN

Sophia's architecture combines a region in-memory index with a
in-memory key index.

A region index is represented as an ordered range of regions with
their min and max keys and a latest on-disk reference. Regions
never overlap.

These regions have the same semantical meaning as the B-Tree
pages, but are designed differently. They do not have a tree
structure or any internal page-to-page relationships and thus no
meta-data overhead (specifically to append-only B-Tree).

A single region on-disk holds keys with values. And as a B-tree
page, region has it's maximum key count. Regions are uniquely
identified by region id number, by which they can be tracked in
future.

A key index is very similar to LSM zero-level (memtable), but has
a different key lifecycle. All modifications first get into the
index and hold until they will be explicitly removed by merger.

The database update lifecycle is organized in terms of epochs.
Epoch lifetime is determined in terms of key updates. When the
update counter reaches an epoch's watermark number then the
Rotation event happen.

Each epoch, depending to its state, is associated with a single
log file or database file. 

Before getting added to the in-memory index, modifications are
first written to the epoch's write-ahead log.

On each rotation event:

a. current epoch, which is called 'live', is marked as 'transfer'
  and a new 'live' epoch is created (new log file)
b. create new and swap current in-memory key index
c. merger thread is being woken up

The merger thread is the core part that is responsible for region
merging and garbage collecting of a old regions and older epochs.
On wakeup, the merger thread iterates through list of epochs
marked as 'transfer' and starts the merge procedure.

The merge procedure has the following steps:

1. create new database file for the latest 'transfer' epoch
2. fetch any keys from the in-memory index that associated with a single
destination region
3. for each fetched key and origin region start the merge and write a new
region to the database file
4. on each completed region (current merged key count is less or equal to
max region key count):
a. allocate new split region for region index, set min and max
b. first region always has id of origin destination region
c. link region and schedule for future commit
5. on origin region update complete:
a. update destination region index file reference to the current epoch
and insert split regions
b. remove keys from key index
6. start step (2) until there is no updates left
7. start garbage collector
8. database synced with disk and if everything went well, remove all
'transfer' epochs (log files) and gc'ed databases
9. free index

Design of garbage collector

All that is needed is to track an epoch's total region count and a
count of transfered regions during merge procedure. Thus, if some
older epoch database has fewer than 70% (or any other changeable
factor) live regions they just get copied to current epoch
database file and the old one is being deleted.

On database recovery, Sophia tracks and builds an index of pages from the
youngest epochs (biggest numbers) down to the oldest. Log files are being
replayed and epochs are marked as 'transfer'.

Sophia has been evaluated as having the following complexity (in terms of
disk accesses):

set: worst case is a O(1) append-only key write + in-memory index insert

get: worst case is a O(1) random region read, which itself do amortized O(log
region_size) key compares + in-memory key index search + in-memory region
search

range: range queries are very fast due to the fact that each iteration needs
to compare no more that two keys without search them, and access through
mmaped database. Roughly complexity can be equally evaluated as having to
sequentially read a mmaped file. 


Conclusion
----------

A data structure is always a trade-off between:
- data freshness
- speed of insert
- spead of look up

On top of that, your access patterns make a difference!
And your actual M, N and B too.

So choose your engine wisely.


Links
-----

Bitcask A Log-Structured Hash Table for Fast Key/Value Data
Justin Sheehy David Smith with inspiration from Eric Brewer

The Log-Structured Merge-Tree (LSM-Tree) Patrick O'Neil , Edward Cheng
Dieter Gawlick, Elizabeth O'Neil

Cache-Oblivious Algorithms by Harald Prokop (Master theses)

Space/time trade-offs in hash coding with allowable errors, Burton
H. Bloom

Data Structures and Algorithms for Big Databases
Michael A. Bender Stony Brook & Tokutek Bradley C. Kuszmaul
(XLDB tutorial)

sphia.org

http://codecapsule.com/2012/12/30/implementing-a-key-value-store-part-3-comparative-analysis-of-the-architectures-of-kyoto-cabinet-and-leveldb/

http://stackoverflow.com/questions/6079890/cache-oblivious-lookahead-array

http://www.youtube.com/watch?v=88NaRUdoWZM
(Tim Callaghan: Fractal Tree indexes)

http://code.google.com/p/leveldb/downloads/list
