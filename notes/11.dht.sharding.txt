Шардинг и проблемы решардинга.
-----------------------------

Шардинг является одним из наиболее распространённых способов 
горизонтального масштабирования. 

Общий принцип шардинга в том, что данные разбиваются по некотрому
принципу на несвязные сегменты, каждый сегмент размещается на одном
из множества серверов кластера. 

Таким образом и нагрузка на СУБД, как по хранению так и по
выполнению запросов, может быть распределена по
узлам кластера, в зависимости от того, с каким сегментом
идёт работа. 

Шардинг в общем случае представляет собой механзим практически
линейного горизонтального масштабирования, при условии
что данные на несвязанных шардах не связаны между собой.

Это условие, однако, не всегда можно выполнить.

Рассмотрим проблему поиска кратчайшего пути в графе.
Представим граф в виде множества вершин, распределённых
по кластеру в соответствии с некоторой шард функцией.
Вне зависимости от выбора шард функции путь может
идти из любой точки графа в любую другую, таким образом 
доступ к данным будет нелокальным при любом варианте размещения.
К счастью, конкретно для графа проблема решается добавлением
некоторой избыточностью хранени яданных.


Похожим образом не решается при разбиении данных по равнозначным
узлам кластера проблема быстрого вычисления аггрегатной
информации. 
Предположим что мы имеем своей задачей разместить ан кластере
отношение покупатели - заказы - склад.

При любом варианте размещения будут существовать запросы, которые
невозможно выполнив при этом обращаясь лишь к локальным данным.

Это фундамендатльная проблема компромисса оптимальности размещения
и оптимальности доступа к данным.

Решение, однако, существует - и оно похоже на решение
принятое в реляционных базах. Один из вариантов решения
описан в H-store papre. Другой, более общий вариант, выглядит следующим
образом - необходимо агрегировать данные разным избыточным образом
для оптимизации аналитических задач.

Давайте забудем на минуту о проблемах Ad-hoc доступа к данным, и предоложим
что необходимо решить лишь одну локальную задачу - оптимально распределить
key-value схему по кластеру. Каким образом выбрать принцип
разбиения данных. 

Т.к. на практике встречаются все способы (и все способы в определённых
обстоятельствах могут быть оптимальны), рассмотрим их в порядке
усложнения.

В случае если данные представляют собой некую естественную
возрастающую последовательность - заказы по датам, автоинкремент, 
и т.п. может возникнуть желание разбивать данные по степени наполняемости
кластера. Какие проблемы при этом могут быть возникнут?

Например, если существует неравномерность обращений к данным, новые
данные являются наиболее популярным, вся нагрузка упадёт на новый шард.
А что если скорость роста неравномерна, каким образом выполнять
provisioning? Это реальная проблема, возникшая в реальной компании -
Twitter, Которая использовала такой способ разбиения для новых твитов.

Когда присходили выборы президента США команда сис. администраторов
работала 24x7 добавляя новые сервера!

Таким образом мы видим что этот способ ведёт за собой совсем
не эластичное деградирование качества работы СУБД.

Следующим способом применяемым на практике является избрание хэш-функции
и разбиение диапазона ключей по значению хэш функции. 

Даже на этапе разбиения следует учитывать факт неабсолютной равномерности
разбиения - то есть наргрузка на разные шарды может варьироваться
до 20% что не всегда допустимо. 

Поэтому иногда на практике используется табличное задание 
размещения по диапазонам, либо по рандомизированным диапазонам,
и перемещение шардов за счёт изменения их позиции в таблице.

О решардинге, однако, стоит говорить отдельно. 

При решардинге системы использующей обычный хэш
может потребоваться изменить размещение каждого ключа, т.к. 
хэш функция начинает выдавать разные результаты в зависимости
от количства шардов.
Иногда это допустимо, например если речь идёт о кэше, и
известно что рано или поздно любой элемент кэша будет обновлён.

Иногда эту проблему можно решить удваивая количество шардов
- таким образом разбивая диапазон каждого шарда (возможно, не сразу)
на два. 

Но фундаментально эта проблема решается только консистентным
хэшированием.

Для того, чтобы понять что такое консистентное хэширование, 
представим множество значений хэш функции как циферблат в диапазоне, 
скажем от 0 до 4 миллиардов.
Разобьём данный диапазон на доатсточно большое (заведомо
много большее чем число машин) количество секторов, скажем 3000
секторов. 
Определим вторую хэш функцию, которая будет по номеру машины
также выдавать её расположение на циферблате. 
Определим, что все сектора расположенные по часовой стрелке
от точки, на которую приходится машина, вплоть до точки
следующей машины, должны обслуживаться этой машиной.

В этом случае, при добавлении машины в клатсер 
она гарантированно разобьёт лишь один диапазон, и количество данных
подележащих перемещению будет пропорционально количеству данных, 
хранящихся на одной машине.

Иногда и этот способ не идеален, т.к. мы хотели бы чтобы новая
машина равномерно разгрузила все машины в кластере, а не только
те, диапазон которых она забирает. В этом случае используется
техника с так называемыми virtual nodes - изначально консистентное
кольцо предразбивается на заведомо большое число нод, скажем
10000. Каждая из виртуальных нод назначается на текущие физические
узлы при помощи хэш-функции f(vnode_id) % server_count.
, таким образом при наличии 10 узлов каждый узел получает 1000
случайных нод. При добавлении 11го узла, каждую
десятую ноду. При добавлении узла на кластер,  
определим вторую функцию, ото, отображающую номер сектора на номер
машины) 2do вспомнить и понять как это работает, блин помню 
только по вторникам.


Однако и этот способ не идеален, т.к. подразумевает перемещение
данных между старыми шардами.

Что если мы хотим чтобы данные *иникогда* не перемещались
между старыми шардами, а только равномерно перераспределялись
на новую машину? В этом случае можно использовать следующий
алгоритм (здесь мы описываем сумбур).

Поделим весь диапазон хэш функции пополам.

Потом возьмём 1/3 от каждой половинки и перенесём на 3й шард
Потом возьмём 1/4 от каждой трети и 1/8 от каждоый 1/6  и перенесём на 4й шард
и так далее.

In his blog post on consistent hashing Tom White has simulated the effect of
adding virtual nodes in a setting where he distributes 10,000 objects across
ten physical nodes. As a result, the standard deviation of the objects
distribution can be dropped from 100% without virtual nodes to 50% with only
2–5 virtual nodes and to 5–10% with 500 virtual nodes per physical node (cf.
[Whi07]).


Как видмим, этот принцип либо должен быть задан таблично, либо с помощью
рекурсивной функции. На практике это достаточно быстрый способ вычисления хэш
функции (при современнойм соотношении скорости вычислений к скорости
коммуникаций).

Преимущество - абсолютно равномерное распределение данных.
Данные никогда не перемещаются между старыми шардами.

Недостаток - вычислительная сложность шард-функции.

Ощё одна задача решаемая консистентным хэшированиеум - размещение
данных более чем на одном узле.


Сссылки по теме:

https://github.com/RJ/ketama
https://github.com/mailru/sumbur-ruby

